# Plano de Evolu√ß√£o para o Sistema de Vagas

Este documento descreve a proposta de evolu√ß√£o do sistema de listagem de vagas do site, migrando de um processo manual para um sistema automatizado que utiliza APIs de terceiros e web scrapers.

## 1. Objetivo

O objetivo principal √© aumentar a quantidade, a qualidade e a atualidade das vagas de emprego listadas no site, ao mesmo tempo em que se reduz o esfor√ßo manual de curadoria e publica√ß√£o.

## 2. Situa√ß√£o Atual

Atualmente, as vagas s√£o gerenciadas de forma est√°tica, provavelmente atrav√©s da edi√ß√£o manual de um arquivo JSON (`src/data/jobs.json`). Este processo √© trabalhoso, escal√°vel de forma limitada e propenso a desatualiza√ß√£o das vagas.

## 3. Arquitetura Proposta

**üéØ UPDATE: Migra√ß√£o para Supabase** (Substituindo Airtable)

A nova arquitetura ser√° baseada em um servi√ßo de agrega√ß√£o de vagas que funcionar√° da seguinte forma:

- **Fontes de Dados:**
    - **APIs Livres:** Conex√£o com APIs de plataformas de emprego que oferecem acesso gratuito aos seus dados.
    - **Web Scrapers:** Rob√¥s que extraem informa√ß√µes de vagas diretamente de p√°ginas de carreiras de empresas ou de outros sites de emprego que n√£o possuem API.

- **Banco de Dados: Supabase (PostgreSQL)**
    - **Substitui Airtable** - Banco de dados robusto com interface visual (Table Editor)
    - **Performance superior** - Sem rate limits artificiais, queries SQL r√°pidas
    - **Deduplica√ß√£o nativa** - Upsert do PostgreSQL garante unicidade
    - **Interface visual mantida** - Table Editor permite gest√£o manual de vagas
    - **Escal√°vel** - Preparado para milhares de vagas sem degrada√ß√£o

- **Orquestrador:**
    - Um script central (orquestrador) ser√° respons√°vel por acionar os conectores de API e os scrapers.
    - Este script poder√° ser executado periodicamente atrav√©s de um agendador (como Cron Job, GitHub Actions ou um servi√ßo serverless).

- **Processamento e Armazenamento:**
    - Os dados coletados ser√£o normalizados para um formato padr√£o.
    - Um mecanismo de deduplica√ß√£o ser√° implementado usando upsert do Supabase.
    - As vagas validadas e processadas ser√£o salvas no **Supabase (PostgreSQL)**.
    - **Gera√ß√£o est√°tica:** As vagas do Supabase ser√£o sincronizadas para `src/data/jobs.json`, mantendo a compatibilidade com o frontend existente.

## 4. Sugest√£o para o MVP (Produto M√≠nimo Vi√°vel)

Para validar a arquitetura proposta com o m√≠nimo de esfor√ßo, o MVP se concentrar√° em:

- **Integra√ß√£o com ATS p√∫blicos:** Integrar APIs de Greenhouse, Ashby e Lever focando em empresas relevantes.
- **Cria√ß√£o de 1 Scraper complementar:** Desenvolver um scraper para Wildlife Studios como fonte adicional (empresa j√° presente no site).
- **Script de Orquestra√ß√£o Manual:** Criar o script que executa a coleta, transforma√ß√£o e salvamento dos dados, mas com acionamento manual.
- **Normaliza√ß√£o e Deduplica√ß√£o:** Implementar l√≥gica b√°sica para normalizar dados de diferentes fontes e evitar duplicatas.
- **Manter o Frontend:** Nenhuma altera√ß√£o ser√° feita na interface do usu√°rio nesta fase. O foco √© validar o backend e garantir que todos os links sejam diretos.

### 4.1. APIs Escolhidas para o MVP: Greenhouse, Ashby e Lever

Ap√≥s uma an√°lise detalhada, foram escolhidas **APIs de ATS p√∫blicos** como fonte de dados prim√°ria para o MVP. Estas plataformas oferecem endpoints JSON p√∫blicos que retornam links diretos para aplica√ß√£o, evitando camadas intermedi√°rias.

#### Por que ATS p√∫blicos (Greenhouse, Ashby, Lever)?

**Vantagens:**
- ‚úÖ **Links diretos para aplica√ß√£o** - Sem intermedi√°rios (candidato vai direto do seu site para a vaga)
- ‚úÖ **Dados estruturados e completos** - T√≠tulo, descri√ß√£o, categoria, localiza√ß√£o, tags, sal√°rio
- ‚úÖ **Atualiza√ß√£o em tempo real** - Vagas refletem o estado atual do board da empresa
- ‚úÖ **Experi√™ncia do usu√°rio superior** - Sem redirecionamentos desnecess√°rios
- ‚úÖ **Controle total** - Voc√™ controla a experi√™ncia do candidato
- ‚úÖ **Ampla ado√ß√£o** - Milhares de empresas usam essas plataformas

#### Greenhouse Board API
- **Endpoint:** `https://boards.greenhouse.io/{company}/jobs.json`
- **Exemplo de empresas:** Automattic, Stripe, Shopify, Reddit, e muitas outras
- **Estrutura:** JSON bem estruturado com todos os campos necess√°rios
- **Documenta√ß√£o:** Formatos p√∫blicos bem documentados

#### Lever API
- **Endpoint:** `https://api.lever.co/v0/postings/{company}` ou endpoints p√∫blicos em p√°ginas de carreiras
- **Exemplo de empresas:** Netflix, Reddit, e outras empresas de tecnologia
- **Estrutura:** Similar ao Greenhouse, formato consistente

#### Ashby
- **Estrutura:** Geralmente endpoints JSON embutidos em p√°ginas de carreiras
- **Vantagens:** Dados estruturados, links diretos
- **Desafio:** Formato pode variar mais entre empresas

#### Por que N√ÉO Remotive (inicialmente)?
- ‚ùå **Cria camada extra** - Seu site ‚Üí Remotive ‚Üí Site da vaga
- ‚ùå **Redirecionamento desnecess√°rio** - Piora a experi√™ncia do usu√°rio
- ‚ùå **Menos controle** - Voc√™ n√£o controla a jornada do candidato
- ‚ùå **Potencial perda de convers√£o** - Cada camada adicional pode reduzir candidaturas
- ‚úÖ **Pode ser considerado depois** - Para vagas que n√£o est√£o dispon√≠veis em ATS p√∫blicos

### 4.2. Estrat√©gia de Scraping para o MVP

- **Biblioteca:** Manteremos o uso do **Playwright**, que j√° est√° configurado no projeto para testes e √© excelente para lidar com sites din√¢micos.
- **Alvo Inicial:** Para o primeiro scraper, sugerimos a p√°gina de carreiras da **Wildlife Studios** (`https://wildlifestudios.com/careers/`). A escolha se baseia no fato de ser uma empresa de tecnologia relevante no cen√°rio brasileiro e j√° possuir recursos visuais no site.

## 5. Pr√≥ximos Passos

1.  **Pesquisa de APIs:** **(Conclu√≠do)** Foco em ATS p√∫blicos (Greenhouse, Ashby, Lever) foi definido ap√≥s an√°lise comparativa.
2.  **Migra√ß√£o para Supabase:** **(PRIORIDADE)** Migrar completamente do Airtable para Supabase.
    - Setup do projeto Supabase
    - Criar schema do banco (jobs, companies, categories, tags, job_tags)
    - Migrar dados existentes do Airtable
    - Reescrever scripts de sincroniza√ß√£o
    - Validar funcionamento completo
3.  **Identifica√ß√£o de Empresas:** Listar empresas relevantes que usam Greenhouse, Ashby ou Lever e que publicam vagas remotas para o mercado brasileiro.
4.  **Desenvolvimento do Orquestrador:** Implementar o script inicial para buscar e processar os dados de m√∫ltiplas fontes (ATS p√∫blicos) no Supabase.
5.  **Implementa√ß√£o dos Adaptadores:**
    - ‚úÖ Criar adaptador para Greenhouse Board API (Conclu√≠do - Wildlife Studios, Automattic, GitLab)
    - ‚úÖ Criar adaptador para Lever API (Conclu√≠do - Fanatee)
    - ‚úÖ Criar adaptador para Ashby GraphQL (Conclu√≠do - Deel, Ashby)
    - ‚è≥ Testar adaptadores com empresas reais
    - ‚è≥ Criar adaptador/scraper para Wildlife Studios (complementar)
6.  **Normaliza√ß√£o e Deduplica√ß√£o:** Implementar l√≥gica usando upsert do Supabase para normalizar dados de diferentes fontes e evitar duplicatas.
7.  **Testes:** Garantir que os dados s√£o coletados, normalizados e salvos corretamente no Supabase e gerados no formato esperado (`src/data/jobs.json`).
8.  **Valida√ß√£o de Links Diretos:** Confirmar que todos os links de aplica√ß√£o s√£o diretos (sem intermedi√°rios).

---

## 6. Documenta√ß√£o Adicional

- **`docs/SUPABASE_MIGRATION.md`** - Plano completo de migra√ß√£o do Airtable para Supabase
- **`docs/FETCHERS_GUIDE.md`** - Guia completo dos fetchers de vagas (Greenhouse, Lever, Ashby)
- **`archive/AIRTABLE_SCHEMA_OPTIMIZED.md`** - Schema anterior (Airtable) - arquivado como refer√™ncia hist√≥rica

## 7. Status Atual da Implementa√ß√£o

### ‚úÖ Conclu√≠do:
- [x] Fetcher Greenhouse (Wildlife Studios, Automattic, GitLab, Monks, AE.Studio)
- [x] Fetcher Lever (Fanatee)
- [x] Fetcher Ashby (Deel, Ashby)
- [x] Sistema de categoriza√ß√£o inteligente (VFX, 3D, 2D Art, Animation, Design, Game Dev)
- [x] Detec√ß√£o de location scope (remote-brazil, remote-latam, remote-worldwide, hybrid, onsite)
- [x] Extra√ß√£o inteligente de tags (AI + fallback)
- [x] Filtros de vagas relevantes
- [x] Documenta√ß√£o completa dos fetchers

### ‚è≥ Em Andamento:
- [ ] Testar Lever com Fanatee (aguardando execu√ß√£o)
- [ ] Testar Ashby com Deel (aguardando execu√ß√£o)
- [ ] Validar outputs e ajustar mapeamentos se necess√°rio

### üìã Pr√≥ximas A√ß√µes:
1. **Testar os fetchers criados:**
   ```bash
   node scripts/fetch-lever-jobs.mjs
   node scripts/fetch-ashby-jobs.mjs
   ```

2. **Revisar outputs:**
   - `scripts/lever-jobs-output.json`
   - `scripts/ashby-jobs-output.json`

3. **Ajustar mapeamentos** se necess√°rio (categorias, location scopes)

4. **Adicionar mais empresas** usando os mesmos fetchers

5. **Criar orquestrador** para executar todos os fetchers

6. **Integrar com Supabase** para salvar vagas no banco

7. **Automatizar** com GitHub Actions (daily sync)
